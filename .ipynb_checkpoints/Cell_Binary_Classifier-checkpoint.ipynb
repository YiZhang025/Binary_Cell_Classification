{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bf11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch,torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import datetime  \n",
    "from torch.utils import data\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable cuda\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "#construct dataset from pandas\n",
    "path = #training data path\n",
    "files = glob.glob(path+'/*.tif')\n",
    "df = pd.DataFrame(files)\n",
    "df.columns = ['Path']\n",
    "df.loc[df['Path'].str.contains('LABELAB30'),'Label'] = 1\n",
    "df.loc[df['Path'].str.contains('LABELVEH00'),'Label'] = 0\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        #I here convert the images into grayscale.\n",
    "        img = Image.open(row[\"Path\"]).convert('L')\n",
    "        label = int(row[\"Label\"])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return (\n",
    "            img,\n",
    "            label,\n",
    "        )\n",
    "    \n",
    "class DatasetFromSubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "tsfm = transforms.Compose([transforms.Resize(512)])\n",
    "dataset = MyDataset(df,transform = tsfm)\n",
    "\n",
    "#transform: the normalize parameters are calculated over the *original* whole dataset\n",
    "tsfm_train = transforms.Compose([transforms.RandomCrop(448),\n",
    "                                 transforms.RandomRotation(30),\n",
    "                                 transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                 transforms.RandomVerticalFlip(p=0.5),\n",
    "                                 transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((.6662),(.2179))\n",
    "                                ])\n",
    "tsfm_val = transforms.Compose([transforms.CenterCrop(448),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((.6662),(.2179))\n",
    "                              ])\n",
    "#divide train,val,test sets\n",
    "n = len(dataset)\n",
    "n_train = int(0.8*n)\n",
    "n_val = int(0.1*n)\n",
    "n_test = int(0.1*n)\n",
    "\n",
    "train_set, val_set, test_set = data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_data = DatasetFromSubset(train_set,transform = tsfm_train)\n",
    "val_data = DatasetFromSubset(val_set,transform = tsfm_val)\n",
    "test_data = DatasetFromSubset(test_set,transform = tsfm_val)\n",
    "\n",
    "#Dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=8,\n",
    "                                           shuffle=True,pin_memory = True)  # <1>\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=8,\n",
    "shuffle=True,pin_memory = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=8,\n",
    "shuffle=True,pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddcc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use resnet18 and change the last layer's output to 2 dimension\n",
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "new_fc= nn.Linear(in_features=512, out_features=2, bias=True,)\n",
    "resnet18.fc = new_fc\n",
    "torch.nn.init.xavier_uniform(resnet18.fc.weight)\n",
    "#set model onto proper device\n",
    "resnet18 = resnet18.to(device)\n",
    "# Freeze selected parts of the architecture\n",
    "for parameter in resnet18.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "for parameter in resnet18.fc.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "#for parameter in resnet18.layer4.parameters():\n",
    "    #parameter.requires_grad = True\n",
    "#change the first layer with 1-channel input and initalize weights.\n",
    "# try different aggregations\n",
    "conv1_init_weight = resnet18.conv1.weight.mean(dim = 1).unsqueeze(dim = 1)\n",
    "resnet18.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "resnet18.conv1.weight.data = conv1_init_weight    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ff521",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "layer_count = 0\n",
    "for parameter in resnet18.children():\n",
    "    parameter.requires_grad = False\n",
    "    layer_count+=1\n",
    "layer_count\n",
    "# Avoid empty optimizer when initialization\n",
    "for parameter in resnet18.fc.parameters():\n",
    "    parameter.requires_grad = True\n",
    "for parameter in resnet18.layer4.parameters():\n",
    "    parameter.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a621e4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(start_epoch+1, 51):\n",
    "    # set models to train mode\n",
    "    resnet18.train()\n",
    "    print(epoch)\n",
    "    i = layer_count\n",
    "    for child in resnet18.children():\n",
    "        i-=1\n",
    "        if i < (epoch // 10): \n",
    "            for parameter in child.parameters():\n",
    "                parameter.requires_grad = True\n",
    "        else:\n",
    "            for parameter in child.parameters():\n",
    "                parameter.requires_grad = False\n",
    "    #from the first epoch, unfreeze last two blocks\n",
    "    for parameter in resnet18.fc.parameters():\n",
    "        parameter.requires_grad = True\n",
    "    for parameter in resnet18.layer4.parameters():\n",
    "        parameter.requires_grad = True\n",
    "    #print(child)\n",
    "    #print(parameter.requires_grad)\n",
    "    # use prefetch_generator and tqdm for iterating through data\n",
    "    pbar = tqdm(enumerate(train_loader),\n",
    "                total=len(train_loader),bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, data in pbar:  \n",
    "        imgs , labels = data\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #track time\n",
    "        prepare_time = start_time-time.time()\n",
    "        \n",
    "        outputs = resnet18(imgs)  \n",
    "        loss = loss_fn(outputs, labels)  \n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()  \n",
    "        optimizer.step()   \n",
    "\n",
    "        # compute computation time and *compute_efficiency*\n",
    "        process_time = start_time-time.time()-prepare_time\n",
    "        compute_efficiency = process_time/(process_time+prepare_time)\n",
    "        pbar.set_description(\n",
    "            f'Compute efficiency: {compute_efficiency:.4f}, ' \n",
    "            f'loss: {loss.item():.4f},  epoch: {epoch}/{30}')\n",
    "        start_time = time.time()\n",
    "                # maybe do a test pass every N=1 epochs\n",
    "    if epoch % 1 == 0:\n",
    "        # bring models to evaluation mode\n",
    "        resnet18.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pbar = tqdm(enumerate(val_loader),\n",
    "                total=len(val_loader),bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') \n",
    "        with torch.no_grad():\n",
    "            for i, data in pbar:\n",
    "                # data preparation\n",
    "                imgs , labels = data\n",
    "                imgs = imgs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                out = resnet18(imgs)\n",
    "                _, predicted = torch.max(out.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy on test set: {100*correct/total:.2f}')\n",
    "        # udpate tensorboardX\n",
    "        writer.add_scalar('Accuracy',correct/total, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cff368",
   "metadata": {},
   "source": [
    "# Test on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e57e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#test set, typo on previous ones, they should be *val* \n",
    "resnet18.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "pbar = tqdm(enumerate(test_loader),\n",
    "        total=len(test_loader),bar_format='{l_bar}{bar:5}{r_bar}{bar:-5b}') \n",
    "with torch.no_grad():\n",
    "    for i, data in pbar:\n",
    "        # data preparation\n",
    "        imgs , labels = data\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        out = resnet18(imgs)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy on test set: {100*correct/total:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32f951",
   "metadata": {},
   "source": [
    "# Grad-CAM on monochrome input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torch.load(\"D:/CellClass/resnet18_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626df78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use new dataset in order to utilize cv2 image\n",
    "tsfm_val_grad = transforms.Compose([transforms.CenterCrop(448),\n",
    "                                 ])\n",
    "val_grad = DatasetFromSubset(val_set,transform = tsfm_val_grad)\n",
    "index = 100\n",
    "img1,label1 = val_data[index]\n",
    "img2,label2 = val_grad[index]\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1.permute(1,2,0))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "rgb_image =numpy.array(img2)\n",
    "rgb_image = cv2.cvtColor(rgb_image, cv2.COLOR_GRAY2RGB)\n",
    "rgb_image = rgb_image/255\n",
    "rgb_image = rgb_image.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = [resnet18.layer1[-1],resnet18.layer2[-1],\n",
    "                resnet18.layer3[-1],resnet18.layer4[-1],\n",
    "               ]\n",
    "layer_name = ['Layer1','Layer2','Layer3','Layer4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1ba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.layer2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594054ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = [resnet18.layer2[0].conv1,resnet18.layer2[-1].conv2,\n",
    "                resnet18.layer3[0].conv1,resnet18.layer3[-1].conv2,\n",
    "                resnet18.layer4[0].conv1,resnet18.layer4[0].conv2,\n",
    "                resnet18.layer4[-1].conv1,resnet18.layer4[-1].conv2,\n",
    "               ]\n",
    "layer_name = ['L2,B1,Conv1','L2,B2,Conv2','L3,B1,Conv1','L3,B2,Conv2','L4,B1,Conv1','L4,B1,Conv2','L4,B2,Conv1','L4,B2,Conv2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116694e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_tensor = img1.unsqueeze(0).to(device)\n",
    "plt.figure(figsize=(22, 10)) \n",
    "for i,layer in enumerate(target_layer):  \n",
    "    cam =XGradCAM(model=resnet18, target_layer=layer)\n",
    "    target_category = 1\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, target_category=target_category)\n",
    "    grayscale_cam= grayscale_cam[0,:]\n",
    "    visualization = show_cam_on_image(rgb_image,grayscale_cam)\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.title(layer_name[i])\n",
    "    plt.imshow(visualization)\n",
    "plt.tight_layout()\n",
    "# Due to my memory, I can only plot these few plots...\n",
    "#plt.savefig('Xgrad_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98042ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = img1.unsqueeze(0).to(device)\n",
    "plt.figure(figsize=(13, 22)) \n",
    "for i,layer in enumerate(target_layer):  \n",
    "    cam =GradCAM(model=resnet18, target_layer=layer)\n",
    "    target_category = label1\n",
    "    grayscale_cam = cam(input_tensor=input_tensor, target_category=target_category)\n",
    "    grayscale_cam= grayscale_cam[0,:]\n",
    "    visualization = show_cam_on_image(rgb_image,grayscale_cam)\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.title(layer_name[i])\n",
    "    plt.imshow(visualization)\n",
    "# Due to my memory, I can only plot these few plots...\n",
    "plt.savefig('grad.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bda37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
